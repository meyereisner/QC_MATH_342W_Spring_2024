---
title: "Lab 8"
author: "Tao Wu"
output: pdf_document
---

#Model Selection with Three Splits: Select from M models

We employ the diamonds dataset and specify M models nested from simple to more complex. We store the models as strings in a list (i.e. a hashset)

```{r}
?ggplot2::diamonds
#model formulas saved as strings
model_formulas = c(
  "carat",
  "carat + cut",
  "carat + cut + color",
  "carat + cut + color + clarity",
  "carat + cut + color + clarity + x + y + z",
  "carat + cut + color + clarity + x + y + z + depth",
  "carat + cut + color + clarity + x + y + z + depth + table",
  "carat * (cut + color + clarity) + x + y + z + depth + table",
  "(carat + x + y + z) * (cut + color + clarity) + depth + table",
  "(carat + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + x + y + z + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + depth + table) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity)",
  "(poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table)) * (cut + color + clarity + poly(carat, 2) + poly(x, 2) + poly(y, 2) + poly(z, 2) + poly(depth, 2) + poly(table, 2) + log(carat) + log(x) + log(y) + log(z) + log(depth) + log(table))"
)
#paste0 does not include space.  Compare to paste()
model_formulas = paste0("price ~ ", model_formulas)
M = length(model_formulas)
```

In order to use the formulas with logs we need to eliminate rows with zeros in those measurements:

```{r}
diamonds_cleaned = ggplot2::diamonds
diamonds_cleaned = diamonds_cleaned[
  diamonds_cleaned$carat > 0 &
  diamonds_cleaned$x > 0 &
  diamonds_cleaned$y > 0 &
  diamonds_cleaned$z > 0 &
  diamonds_cleaned$depth > 0 &
  diamonds_cleaned$table > 0, #all columns
]
```

Split the data into train, select and test. Each set should have 1/3 of the total data.

```{r}
n = nrow(diamonds_cleaned)
set.seed(1)
train_idx = sample(1 : n, round(n / 3))
select_idx = sample(setdiff(1 : n, train_idx), round(n / 3))
test_idx = setdiff(1 : n, c(train_idx, select_idx))
diamonds_train =  diamonds_cleaned[train_idx, ]
diamonds_select = diamonds_cleaned[select_idx, ]
diamonds_test =   diamonds_cleaned[test_idx, ]
```

Find the oosRMSE on the select set for each model. Save the number of df in each model while you're doing this as we'll need it for later.

```{r}
dfs = array(NA, M)
# df = degrees of freedom
#M stores 17 from line 35 above
oosRMSEs = array(NA, M)
for(m in 1:M){
  mod = lm(model_formulas[m], diamonds_train)
  dfs[m] = mod$rank
  y_hat = predict(mod, diamonds_select)
  oosRMSEs[m] = sqrt(mean((diamonds_select$price - y_hat)^2))
  #warning about prediction resulting from complicated models
}
```

Plot the oosRMSE by model complexity (df in model)

```{r}
pacman::p_load(ggplot2)
ggplot(data.frame(df = dfs, oosRMSE = oosRMSEs))+
  aes(x = df, y = oosRMSE)+
  geom_line()+
  geom_point()+
#plot should be a U-shape
#scales need to be adjusted (without ylim below, shape appears to be a line)
  ylim(0,10000)

#this should be the graph that shows overfitting big time leads to big oosRMSE , U shape is correct
```

Select the best model by oosRMSE and find its oosRMSE on the test set.

```{r}
m_star = which.min(oosRMSEs)
mod_star = lm(model_formulas[m_star], rbind(diamonds_train, diamonds_select))
y_hat = predict(mod_star, diamonds_test)
sqrt(mean((diamonds_test$price - y_hat)^2))
#did we overfit the select set?
#we did not, but we do need to check
oosRMSEs[m_star]

```

Did we overfit the select set? Discuss why or why not.

No, because the RMSE on the select is more than the select on the test.That is, mod-star did better than mod_select in terms of RMSE.

Create the final model object `g_final`.

```{r}
mod_star = lm(model_formulas[m_star], diamonds_cleaned)
g_final = function(x_star){
  predict(mod_star, x_star)
}
# this is how we ship the g_final
```


#Model Selection with Three Splits: Hyperparameter selection

We will use an algorithm that I historically taught in 342W but now moved to 343 so I can teach it more deeply using the Bayesian topics from 341. The regression algorithm is called "ridge" and it involves solving for the slope vector via:

b_ridge := (X^T X + lambda I_(p+1))^-1 X^T y
#notice that if you remove `+ lambda I_(p+1)^-1` you have OLS
#value of lambda is whatever you choose.  If you choose 0 you have OLS

Note how if lambda = 0, this is the same algorithm as OLS. If lambda becomes very large then b_ridge is pushed towards all zeroes. So ridge is good at weighting only features that matter.

However, lambda is a hyperparameter >= 0 that needs to be selected.
#negative values do not make sense

We will work with the boston housing dataset except we will add 250 garbage features consisting of iid N(0,1) realizations. We will also standardize the columns so they're all xbar = 0 and s_x = 1. This is shown to be important in 343.

```{r}
rm(list = ls())
?MASS::Boston
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
n = nrow(X)
p_garbage = 250
set.seed(1)
X = cbind(X, matrix(rnorm(n * p_garbage), nrow = n, ncol = p_garbage))
?matrix
#standardize feature values
X = apply(X, 2, function(x_dot_j){
                  (x_dot_j - mean(x_dot_j)) / sd(x_dot_j)
                })
X[, 1] = 1
dim(X)
#features that matter are the first 14
```


Now we split it into 300 train, 100 select and 106 test. 

```{r}
set.seed(1)
train_idx = sample(1 : n, 300)
select_idx = sample(setdiff(1 : n, train_idx), 100)
test_idx = setdiff(1 : n, c(train_idx, select_idx))
X_train = X[train_idx, ]
y_train = y[train_idx]
X_select = X[select_idx, ]
y_select = y[select_idx]
X_test = X[test_idx, ]
y_test = y[test_idx] 

```

We now create a grid of M = 200 models indexed by lambda. The lowest lambda should be zero (which is OLS) and the highest lambda can be 100.

```{r}
M = 200
lambda_grid = seq(from = 0, to = 100, length.out = M)
?seq
```

Now find the oosRMSE on the select set on all M models each with their own lambda value.

```{r}
# this is the formula for b_ridge := (X^T X + lambda I_(p+1))^-1 X^T y
# models with different lambda values are considered different models
oosRMSEs = array(NA, M)
XtX = t(X_train) %*% X_train
Xty = t(X_train) %*% y_train
I_p_plus_one = diag(ncol(X_train))
for(m in 1:M){
  b_ridge = solve(XtX + lambda_grid[m] * I_p_plus_one) %*% Xty
  y_hat = X_select %*% b_ridge
  oosRMSEs[m] = sqrt(mean((y_select - y_hat)^2))
}
```

Plot the oosRMSE by the value of lambda.

```{r}
pacman::p_load(ggplot2)
ggplot(data.frame(lambda = lambda_grid, oosRMSE = oosRMSEs))+
  aes(x = lambda, y = oosRMSE)+
  geom_line()+
  geom_point(col = "red")
```


Select the model with the best oosRMSE on the select set and find its oosRMSE on the test set.

```{r}
m_star = which.min(oosRMSEs)
X_train_new = rbind(X_train, X_select)
y_train_and_select = c(y_train, y_select)
XtX = t(X_train_new) %*% X_train_new
Xty = t(X_train_new) %*% y_train_and_select
I_p_plus_one = diag(ncol(X_train_new))

b_ridge_star = solve(XtX + lambda_grid[m_star] * I_p_plus_one) %*% Xty
y_hat_test = X_test %*% b_ridge_star
sqrt(mean((y_test - y_hat_test)^2))
```

Create the final model object `g_final`.

```{r}
b_ridge_star = solve(XtX + lambda_grid[m_star] * I_p_plus_one) %*% Xty
g_final = function(x_star){
  x_star %*% b_ridge_star
}
# this is how we ship the g_final
```


#Model Selection with Three Splits: Forward stepwise modeling

We will use the adult data

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata") #load from github
data(adult)
adult = na.omit(adult) #remove any observations with missingness
n = nrow(adult)
?adult
#let's remove "education" as its duplicative with education_num
adult$education = NULL
```


To implement forward stepwise, we need a "full model" that contains anything and everything we can possible want to use as transformed predictors. Let's first create log features of all the numeric features. Instead of pure log, use log(value + 1) to handle possible zeroes.

```{r}
skimr::skim(adult)
#this gives us the list of numeric features to create logs
adult$log_age = log(adult$age + 1)
adult$log_fnlwgt = log(adult$fnlwgt + 1)
adult$log_education_num = log(adult$education_num + 1)
adult$log_capital_gain = log(adult$capital_gain + 1)
adult$log_capital_loss = log(adult$capital_loss + 1)
adult$log_hours_per_week = log(adult$hours_per_week + 1)
dim(adult)
```

Now let's create a model matrix Xfull that contains all first order interactions. How many degrees of freedom in this "full model"?

```{r}
?model.matrix
Xfull = model.matrix(income~. * ., adult)
p_plus_one = ncol(Xfull)
```

Now let's split it into train, select and test sets. Because this will be a glm, model-building (training) will be slow, so let's keep the training set small at 2,000. Since prediction is fast, we can divide the others evenly among select and test.

```{r}
y = ifelse(adult$income == ">50K", 1, 0)
train_idx = sample(1: n, 2000)
select_idx = sample(setdiff(1:n, train_idx), 14000)
test_idx = setdiff(1:n, c(train_idx, select_ids))
Xfull_train =  Xfull[train_idx, ]
Xfull_select = Xfull[select_ids, ]
Xfull_test =   Xfull[test_idx, ]
y_train =      y[train_idx]
y_select =     y[select_idx]
y_test =       y[test_idx]
```

Now let's use the code from class to run the forward stepwise modeling. As this is binary classification, let's use logistic regression and to measure model performance, let's use the Brier score. Compute the Brier score in-sample (on training set) and oos (on selection set) for every iteration of j, the number of features selected from the greedy selection procedure.

```{r}
included_features_by_iter = c() #keep a growing list of predictors by iteration
in_sample_briers_by_iteration = c() #keep a growing list of se's by iteration
oos_briers_by_iteration = c() #keep a growing list of se's by iteration
i = 1
Xmm_train = Xfull_train
Xmm_select = Xfull_select
Xmm_test = Xfull_test
# repeat {
#   all_briers = array(NA, p_plus_one)
#   
#   for(j_try in 1:p_plus_one){
#     
#     if (j_try %in% included_features_by_iter){
#       next
#     }
#     Xmm_sub = Xmm_train[,c(included_features_by_iter, j_try), drop=FALSE]
#     log_mod = glm(y_train ~ 0 + Xmm_sub, family = "binomial")
#     phat_train = predict(log_mod, Xmm_sub, type = "response")
#     
#     all_briers[j_try] = -mean(-(ytrain-phat_train)^2)
#   }
#   
#   jstar = which.max(all_briers)
#   
#   included_features_by_iter =  c(predictor_by_iteration, jstar)
#   
#   in_sample_briers_by_iteration = c(in_sample_briers_by_iteration, jstar)
#   
#   
#   Xmm_sub = Xmm_train[,included_features_by_iter, drop = FALSE]
#   log_mod = glm(y_train ~ 0 + Xmm_sub, family = "binomial")
#   #log_mod = glm(y_select ~., Xmm_sub, family = "binomial")
#   phat_select = predict(log_mod, Xmm_sub, type = "response")
#   oos_brier = mean(-(y_select-phat_select)^2)
#   
#   oos_briers_by_iteration = c(oos_briers_by_iteration, oos_brier)
#   if (i > Nsplitsize || i > p_plus_one){
#     break
#   }
# }

repeat {

  #get all predictors left to try
  all_briers = array(NA, p_plus_one) #record all possibilities
  for (j_try in 1 : p_plus_one){
    if (j_try %in% included_features_by_iter){
      next
    }
    Xmm_sub = Xmm_train[, c(included_features_by_iter, j_try), drop = FALSE]
    mod = glm(y_train ~ 0 + Xmm_sub, family = "binomial")
    p_hat = 1 / 1 + exp(Xmm_sub %*% coef(mod))
    #p_hat = predict(mod, Xmm_sub, type = "response") # type = "response" gives us us back p _hat
    all_briers[j_try] = - sum((y_train - p_hat)^2)
    #all_briers[j_try] = sd(lm.fit(Xmm_sub, y_train)$residuals) #glm so much faster than lm!
  }
  j_star = which.max(all_briers)
  included_features_by_iter = c(included_features_by_iter, j_star)
  in_sample_briers_by_iteration = c(in_sample_briers_by_iteration, all_briers[j_star])

  #now let's look at oos briers
  Xmm_sub = Xmm_train[, included_features_by_iter, drop = FALSE]
  mod = glm(y_train ~ 0 + Xmm_sub, family = "binomial")
  p_hat_select = 1/ 1 + exp(Xmm_select[, included_features_by_iter, drop = FALSE] %*% mod$coefficients)
  oos_briers = sum(-((y_select - p_hat_select)^2))
  oos_briers_by_iteration = c(oos_briers_by_iteration, oos_briers)

  cat("i =", i, "in sample: briers_se = ", round(all_briers[j_star], 1), "oos_briers_se", round(oos_briers, 1), "added:", colnames(Xmm_train)[j_star], "\n")

  i = i + 1

  if (i == 7){
    break #why??
  }
}
print("number of features included by iteration")
included_features_by_iter
print("in-sample briers by iteration")
in_sample_briers_by_iteration
print("out-of-sample briers by iteration")
oos_briers_by_iteration
```

Plot the in-sample Brier score (in red) and oos Brier score (in blue) by the number of features used.

```{r}
sim = data.frame(
  iteration = 1: length(in_sample_briers_by_iteration),
  in_sample_briers_by_iteration = in_sample_briers_by_iteration,
  oos_briers_by_iteration = oos_briers_by_iteration
)


ggplot(sim)+
  geom_line(aes(x=iteration, y= in_sample_briers_by_iteration), color = "blue")+
  geom_line(aes(x=iteration, y = oos_briers_by_iteration), color = "red")
```

Select the model with the best oos Brier score on the select set and find its oos Brier score on the test set.

```{r}
#TO-DO
```

Create the final model object `g_final`.

```{r}
#To-Do
```


# Data Wrangling / Munging / Carpentry

Throughout this assignment you should use `dplyr` with `magrittr` piping. I'll be writing the data.table code for you after you're done so you can see it as it may be useful for your future.

```{r}
pacman::p_load(tidyverse, magrittr, data.table)
```

Load the `storms` dataset from the `dplyr` package and read about it using `?storms` and summarize its data via `skimr:skim`. 

```{r}
storms = dplyr::storms
?storms
skimr::skim(storms)
head(storms)
```

To make the modeling exercise easier, let's eliminate rows that have missingness in `tropicalstorm_force_diameter` or `hurricane_force_diameter`.

```{r}
storms = #TO-DO
skimr::skim(storms)
```

Which column(s) should be converted to type factor? Do the conversion:

```{r}
#TO-DO
```

Reorder the columns so name is first, status is second, category is third and the rest are the same.

```{r}
#TO-DO
```

Find a subset of the data of storms only in the 1970's.

```{r}
#TO-DO
```

Find a subset of the data of storm observations only with category 4 and above and wind speed 100MPH and above.

```{r}
#TO-DO
```

Create a new feature `wind_speed_per_unit_pressure`.

```{r}
#TO-DO
```

Create a new feature: `average_diameter` which averages the two diameter metrics. If one is missing, then use the value of the one that is present. If both are missing, leave missing.

```{r}
#TO-DO
```


For each storm, summarize the maximum wind speed. "Summarize" means create a new dataframe with only the summary metrics you care about.

```{r}
#TO-DO
```

Order your dataset by maximum wind speed storm but within the rows of storm show the observations in time order from early to late.

```{r}
#TO-DO
```

Find the strongest storm by wind speed per year.

```{r}
distinct(storms[, max_wind_by_year := max(wind), by = year][wind == max_wind_by_year, .(year, name, wind)])[, .(year, name)]

storms %>%
  group_by(year) %>%
  filter(wind == max(wind)) %>%
  select(year, name, wind) %>%
  distinct %>%
  select(year, name)
```

For each named storm, find its maximum category, wind speed, pressure and diameters. Do not allow the max to be NA (unless all the measurements for that storm were NA).

```{r}
#TO-DO
```


For each year in the dataset, tally the number of storms. "Tally" is a fancy word for "count the number of". Plot the number of storms by year. Any pattern?

```{r}
data(storms)
storms %>% 
  group_by(year) %>% 
  summarize(num_storms = n_distinct(name))

```

For each year in the dataset, tally the storms by category.

```{r}
#TO-DO
```

For each year in the dataset, find the maximum wind speed per status level.

```{r}
#TO-DO
```

For each storm, summarize its average location in latitude / longitude coordinates.

```{r}
#TO-DO
```

For each storm, summarize its duration in number of hours (to the nearest 6hr increment).

```{r}
#TO-DO
```

For storm in a category, create a variable `storm_number` that enumerates the storms 1, 2, ... (in date order).

```{r}
#TO-DO
```

Convert year, month, day, hour into the variable `timestamp` using the `lubridate` package. Although the new package `clock` just came out, `lubridate` still seems to be standard. Next year I'll probably switch the class to be using `clock`.

```{r}
#TO-DO
```

Using the `lubridate` package, create new variables `day_of_week` which is a factor with levels "Sunday", "Monday", ... "Saturday" and `week_of_year` which is integer 1, 2, ..., 52.

```{r}
#TO-DO
```

For each storm, summarize the day in which is started in the following format "Friday, June 27, 1975".

```{r}
#TO-DO
```

Create a new factor variable `decile_windspeed` by binning wind speed into 10 bins.

```{r}
#TO-DO
```

Create a new data frame `serious_storms` which are category 3 and above hurricanes.

```{r}
#TO-DO
```

In `serious_storms`, merge the variables lat and long together into `lat_long` with values `lat / long` as a string.

```{r}
#TO-DO
```

Let's return now to the original storms data frame. For each category, find the average wind speed, pressure and diameters (do not count the NA's in your averaging).

```{r}
#TO-DO
```

For each named storm, find its maximum category, wind speed, pressure and diameters (do not allow the max to be NA) and the number of readings (i.e. observations).

```{r}
#TO-DO
```

Calculate the distance from each storm observation to Miami in a new variable `distance_to_miami`. This is very challenging. You will need a function that computes distances from two sets of latitude / longitude coordinates. 

```{r}
MIAMI_LAT_LONG_COORDS = c(25.7617, -80.1918)
#TO-DO
```

For each storm observation, use the function from the previous question to calculate the distance it moved since the previous observation.

```{r}
#TO-DO
```

For each storm, find the total distance it moved over its observations and its total displacement. "Distance" is a scalar quantity that refers to "how much ground an object has covered" during its motion. "Displacement" is a vector quantity that refers to "how far out of place an object is"; it is the object's overall change in position.

```{r}
#TO-DO
```

For each storm observation, calculate the average speed the storm moved in location.

```{r}
#TO-DO
```

For each storm, calculate its average ground speed (how fast its eye is moving which is different from windspeed around the eye).

```{r}
#TO-DO
```

Is there a relationship between average ground speed and maximum category attained? Use a dataframe summary (not a regression).

```{r}
#TO-DO
```

Now we want to transition to building real design matrices for prediction. This is more in tune with what happens in the real world. Large data dump and you convert it into $X$ and $y$ how you see fit.

Suppose we wish to predict the following: given the first three readings of a storm, can you predict its maximum wind speed? Identify the `y` and identify which features you need $x_1, ... x_p$ and build that matrix with `dplyr` functions. This is not easy, but it is what it's all about. Feel free to "featurize" as creatively as you would like. You aren't going to overfit if you only build a few features relative to the total 198 storms.

```{r}

```

Fit your model. Validate it. 
 
```{r}
#TO-DO
```

Assess your level of success at this endeavor.

#TO-DO


# More data munging with table joins


```{r}
pacman::p_load(tidyverse, magrittr, data.table)
```

We will be using the `storms` dataset from the `dplyr` package. Filter this dataset on all storms that have no missing measurements for the two diameter variables, "tropicalstorm_force_diameter" and "hurricane_force_diameter". Zeroes count as missing as well.

```{r}
#TO-DO
```

From this subset, create a data frame that only has storm name, observation period number for each storm (i.e., 1, 2, ..., T) and the "tropicalstorm_force_diameter" and "hurricane_force_diameter" metrics.

```{r}
#TO-DO
```

Create a data frame in long format with columns "diameter" for the measurement and "diameter_type" which will be categorical taking on the values "hu" or "ts".

```{r}
#TO-DO
```

Using this long-formatted data frame, use a line plot to illustrate both "tropicalstorm_force_diameter" and "hurricane_force_diameter" metrics by observation period for four random storms using a 2x2 faceting. The two diameters should appear in two different colors and there should be an appropriate legend.

```{r}
#TO-DO
```
