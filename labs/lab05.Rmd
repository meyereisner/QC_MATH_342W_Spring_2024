---
  title: "Lab 5 MATH 342W"
author: "Joshua Eisner"
output: pdf_document
date: "11:59PM March 12"
---
  
  Write a function spec'd as follows:

```{r}
#' Orthogonal Projection
#'
#' Projects vector a onto v.
#'
#' @param a   the vector to project
#' @param v   the vector projected onto
#'
#' @returns   a list of two vectors, the orthogonal projection parallel to v named a_parallel, 
#'            and the orthogonal error orthogonal to v called a_perpendicular
orthogonal_projection = function(a, v){
  
  a_parallel = (t(t(v)) %*% t(v) / sum(v^2)) %*% t(t(a))
  list(a_parallel = a_parallel,a_perpendicular = t(t(a)) - a_parallel)
}
```

Provide predictions for each of these computations and then run them to make sure you're correct.

```{r}
orthogonal_projection(c(1,2,3,4), c(1,2,3,4))
#prediction:
orthogonal_projection(c(1, 2, 3, 4), c(0, 2, 0, -1))
#prediction:
result = orthogonal_projection(c(2, 6, 7, 3), c(1, 3, 5, 7))
t(result$a_parallel) %*% result$a_perpendicular
#prediction:
result$a_parallel + result$a_perpendicular
#prediction:
result$a_parallel / c(1, 3, 5, 7)
#prediction:
```



Create a vector y by simulating n = 100 standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the R^2 of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
n = 100
y = rnorm(n)
X = cbind(1, rnorm(n))
yhat = X %*% solve(t(X) %*% X) %*% t(X) %*% y
ybar = mean(y)

SSR = sum((yhat-ybar)^2)
SST = sum((y-ybar)^2)
RSQ = SSR / SST
```

Write a  loop to each time bind a new column of 100 standard iid normals to the matrix X and find the R^2 each time until the number of columns is 100. Create a vector to save all R^2's. What happened??
  
  ```{r}
rsqs = array(NA, n)
rsqs[2] = RSQ
for(i in 3:n){
  X = cbind(X, rnorm(n))
  yhat = X %*% solve(t(X) %*% X) %*% t(X) %*% y
  ybar = mean(y)
  
  SSR = sum((yhat-ybar)^2)
  SST = sum((y-ybar)^2)
  rsqs[i] = SSR / SST
}
rsqs
```

Test that the projection matrix onto this X is the same as I_n. You may have to vectorize the matrices in the `expect_equal` function for the test to work.

```{r}
pacman::p_load(testthat)
expect_equal(X %*% solve(t(X) %*% X) %*% t(X), diag(n))
```

Add one final column to X to bring the number of columns to 101. Then try to compute R^2. What happens? 
  
  ```{r}
X = cbind(X, rnorm(n))
yhat = X %*% solve(t(X) %*% X) %*% t(X) %*% y
```

Why does this make sense?
  
  #TO-DO
  
  Let's use the Boston Housing Data for the following exercises

```{r}
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
p_plus_one = ncol(X)
n = nrow(X)
```

Using your function `orthogonal_projection` orthogonally project onto the column space of X by projecting y on each vector of X individually and adding up the projections and call the sum `yhat_naive`.

```{r}
yhat_naive = matrix(0, nrow = n, ncol =1)
for(i in 1:p_plus_one){
  yhat_naive = yhat_naive + orthogonal_projection(y, X[,i])$a_parallel
}
```

How much double counting occurred? Measure the magnitude relative to the true LS orthogonal projection.

```{r}
yhat = lm(medv ~ ., MASS::Boston)$fitted.values
sqrt(sum(yhat_naive^2)) / sqrt(sum(yhat^2))
```

Is this ratio expected? Why or why not?

#TO-DO

Convert X into V where V has the same column space as X but has orthogonal columns. You can use the function `orthogonal_projection`. This is the Gram-Schmidt orthogonalization algorithm (part A).

```{r}
V = matrix(NA, nrow = n, ncol = p_plus_one)
V[, 1] = X[, 1]
for (j in 2 : ncol(X)){
  V[, j] = X[, j]
  
  for (k in 1:(j-1)){
    v_k = V[, k]
    V[, j] = V[, j, drop = FALSE] - orthogonal_projection(X[,j], v_k)$a-parallel
  }
}
```

Convert V into Q whose columns are the same except normalized. This is the Gram-Schmidt orthogonalization algorithm (part B).

```{r}
Q = matrix(NA, nrow = n, ncol = p_plus_one)
for(j in 1 : p_plus_one){
  Q[, j] = V[, j] / sqrt(sum(V[, j]^2))
}
rm(V)
```

Verify Q^T Q is I_{p+1} i.e. Q is an orthonormal matrix.

```{r}
expect_equal(t(Q) %*% Q, diag(p_plus_one))
```

Is your Q the same as what results from R's built-in QR-decomposition function?
  
  ```{r}
Q_from_Rs_builtin = qr.Q(qr(X))
expect_equal(Q, Q_from_Rs_builtin)
```

Is this expected? Why did this happen?
  
  Yes because the the orthonormal basis is not unique, our orthogonal pick won't be the computer's pick.

Project y onto colsp[Q] and verify it is the same as the OLS fit. You may have to use the function `unname` to compare the vectors since they the entries will likely have different names.

```{r}
yhat_Q = Q%*%t(Q)%*%y
yhat_from_R = yhat = Q_from_Rs_builtin%*%t(Q_from_Rs_builtin)%*%y
expect_equal(yhat_Q, yhat_from_R)
expect_equal(yhat_Q, yhat)
```

Project y onto colsp[Q] one by one and verify it sums to be the projection onto the whole space.

```{r}
yhat_naive = #TO-DO
  ```

Split the Boston Housing Data into a training set and a test set where the training set is 80% of the observations. Do so at random.

```{r}
K = 5
n_test = round(n * 1 / K)
n_train = n - n_test
train_indices = sample(1:n, n_train, replace = FALSE)
X_train = X[train_indices,]
y_train =y[train_indices]
test_indeces = setdiff(1:n, train_indices)
X_test = X[test_indeces,]
y_test = y[test_indeces]
```

Fit an OLS model. Find the s_e in sample and out of sample. Which one is greater? Note: we are now using s_e and not RMSE since RMSE has the n-(p + 1) in the denominator not n-1 which attempts to de-bias the error estimate by inflating the estimate when overfitting in high p. Again, we're just using `sd(e)`, the sample standard deviation of the residuals.

```{r}
b = solve(t(X_train) %*% X_train) %*% t(X_train) %*% y_train
yhat_train = X_train %*% b
e_train = y_train - yhat_train
RMSE_train = sd(e_train)
yhat_test = X_test %*% b
e_test = y_test - yhat_test
RMSE_test = sd(e_test)
```

Do these two exercises `Nsim = 1000` times and find the average difference between s_e and ooss_e. 

```{r}
Nsim = 1000
RMSE_trains = array(NA, Nsim)
RMSE_tests = array(NA, Nsim)
for(nsim in 1:Nsim){
  
  train_indices = sample(1:n, n_train, replace = FALSE)
  X_train = X[train_indices,]
  y_train =y[train_indices]
  test_indices = setdiff(1:n, train_indices)
  X_test = X[test_indeces,]
  y_test = y[test_indeces]
  
  b = solve(t(X_train) %*% X_train) %*% t(X_train) %*% y_train
  yhat_train = X_train %*% b
  e_train = y_train - yhat_train
  RMSE_trains[nsim] = sd(e_train)
  yhat_test = X_test %*% b
  e_test = y_test - yhat_test
  RMSE_tests[nsim] = sd(e_test)
}
mean(RMSE_trains)
mean(RMSE_tests)
```

We'll now add random junk to the data so that `p_plus_one = n_train` and create a new data matrix `X_with_junk.`

```{r}
X_with_junk = cbind(X, matrix(rnorm(n * (n_train - p_plus_one)), nrow = n))
dim(X)
dim(X_with_junk)
```

Repeat the exercise above measuring the average s_e and ooss_e but this time record these metrics by number of features used. That is, do it for the first column of `X_with_junk` (the intercept column), then do it for the first and second columns, then the first three columns, etc until you do it for all columns of `X_with_junk`. Save these in `s_e_by_p` and `ooss_e_by_p`.


```{r}
s_e_by_p = array(NA, n_train)
ooss_e_by_p = array(NA, n_train)
X_with_junk_train = X_with_junk[train_indices, ]
y_train = y[train_indices]
X_with_junk_test = X_with_junk[test_indices, ]
y_test = y[test_indices]

for(j in 1:n_train){
  
  X_with_junk_train_j = X_with_junk_train[ , 1:j, drop = FALSE]
  b = solve(t(X_with_junk_train_j) %*% X_with_junk_train_j) %*% t(X_with_junk_train_j) %*% y_train
  yhat_train = X_with_junk_train_j %*% b
  e_train = y_train - yhat_train
  s_e_by_p[j] = sd(e_train)
  yhat_test = X_with_junk_test[ , 1:j, drop = FALSE] %*% b
  e_test = y_test - yhat_test
  ooss_e_by_p[j] = sd(e_test)
}
```

You can graph them here:
  
  ```{r}
pacman::p_load(ggplot2)
ggplot(
  rbind(
    data.frame(s_e = s_e_by_p, p = 1 : n_train, series = "in-sample"),
    data.frame(s_e = ooss_e_by_p, p = 1 : n_train, series = "out-of-sample")
  )) +
  geom_line(aes(x = p, y = s_e, col = series))
```


```{r}
K = 5
# oos_e_by_p_k = matrix(NA, nrow = n, ncol = n) #save all residuals here - each row are the residuals for number of features = j
set.seed(1984)
temp = rnorm(n)
folds_vec = cut(temp, breaks = quantile(temp, seq(0, 1, length.out = K + 1)), include.lowest = TRUE, labels = FALSE)
rm(temp)
head(folds_vec, 100)
min_n_train = min(n - table(folds_vec))
ooss_e_by_p = array(NA, n_train)
oos_se_by_p_by_k = matrix(NA, nrow = min_n_train, ncol = K)
for (j in 1:min_n_train){
  all_e_test = array(NA, n)
  for(k in 1:K){
    test_indices = which(folds_vec == k)
    train_indices = setdiff(1:n, test_indices)
    X_with_junk_train = X_with_junk[train_indices, ]
    y_train = y[train_indices]
    X_with_junk_test = X_with_junk[test_indices, ]
    y_test = y[test_indices]
    X_with_junk_train_j = X_with_junk_train[ , 1:j, drop = FALSE]
    b = solve(t(X_with_junk_train_j) %*% X_with_junk_train_j) %*% t(X_with_junk_train_j) %*% y_train
    y_hat_test = X_with_junk_test[ , 1:j, drop = FALSE] %*% b
    e_k = y_test - y_hat_test
    all_e_test[test_indices] = e_k
    oos_se_by_p_by_k[j , k] = sd(e_k)
  }
  ooss_e_by_p[j] = sd(all_e_test)
}
 pacman::p_load(ggplot2)
 ggplot(data.frame(
    s_e = ooss_e_by_p,
    #we are taking the sd over all n oos residuals
    p = 1 : n_train
  )) +
  geom_line(aes(x = p, y = s_e))
  dim(oos_se_by_p_by_k)
s_s_es_by_j = apply(oos_se_by_p_by_k, 1, sd)
cbind(
  ooss_e_by_p - 2 * s_s_es_by_j / sqrt(K),
  ooss_e_by_p + 2 * s_s_es_by_j / sqrt(K)
)

```

Even though the concept of confidence intervals (CIs) will not be on the midterm, construct 95% CIs for each of the oosRMSE measurements by number of features, p. A CI is a real-number interval with a lower bound and upper bound. The formula for the CI is [s_e - 2 * s_s_e, s_e + 2 * s_s_e].


```{r}
#TODO
```