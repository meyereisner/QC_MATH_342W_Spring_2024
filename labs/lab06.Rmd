---
title: "Lab 6"
author: "Joshua Eisner"
output: pdf_document
---
#Visualization with the package ggplot2

I highly recommend using the [ggplot cheat sheet](https://rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf) as a reference resource. You will see questions that say "Create the best-looking plot". Among other things you may choose to do, remember to label the axes using real English, provide a title and subtitle. You may want to pick a theme and color scheme that you like and keep that constant throughout this lab. The default is fine if you are running short of time.

Load up the `GSSvocab` dataset in package `carData` as `X` and drop all observations with missing measurements. This will be a very hard visualization exercise since there is not a good model for vocab.

```{r}
pacman::p_load(carData)
X = carData::GSSvocab
X = na.omit(X)
skimr::skim(X)
```
Briefly summarize the documentation on this dataset. What is the data type of each variable? What do you think is the response variable the collectors of this data had in mind?

We have 5 factor and 3 numeric variables in this data set. I think the the collectors could have in mind is the "vocab" variable. This data set feels suited to examination on the relationship between age, educational level, and gender and vocabulary volume.
Create two different plots and identify the best-looking plot you can to examine the `age` variable. Save the best looking plot as an appropriately-named PDF.
```{r}

pacman::p_load(ggplot2)
plot_age = ggplot(X) + aes(age)
plot_age + geom_histogram() # histogram
plot_age + geom_dotplot() # dotplot
plot_age + geom_density() # density
plot_age + geom_bar() # bar graph
```
Create two different plots and identify the best looking plot you can to examine the `vocab` variable. Save the best looking plot as an appropriately-named PDF.
```{r}
plot_vocab = ggplot(X) + aes(vocab)
plot_vocab + geom_histogram() # histogram
plot_vocab + geom_dotplot() # dotplot
plot_vocab + geom_density() # density
X$vocab_factor = factor(X$vocab) # created a new variable that is now categorical
ggplot(X) + aes(vocab_factor) + geom_bar()
```
Create the best-looking plot you can to examine the `ageGroup` variable by `gender`. Does there appear to be an association? There are many ways to do this.

```{r}
# ageGroup is an ordinal categorical variable and gender in this data set is binary
ggplot(X) + aes(x = ageGroup) + geom_bar() + facet_grid(gender ~ .) 
```
Create the best-looking plot you can to examine the `vocab` variable by `age`. Does there appear to be an association?

```{r}
ggplot(X) + aes(x = age, y = vocab_factor) + geom_boxplot()
```
Add an estimate of $f(x)$ using the smoothing geometry to the previous plot. Does there appear to be an association now?

```{r}
ggplot(X) + aes(x = age, y = vocab) + geom_point() + geom_smooth() # smoothing model 
```
Using the plot from the previous question, create the best looking plot overloading with variable `gender`. Does there appear to be an interaction of `gender` and `age`?
```{r}
ggplot(X) + aes(x = age, y = vocab) + geom_point() + geom_smooth() + facet_grid(. ~ gender)
ggplot(X) + aes(x = age, y = vocab, color=gender) + geom_smooth() 
```
Using the plot from the previous question, create the best looking plot overloading with variable `nativeBorn`. Does there appear to be an interaction of `nativeBorn` and `age`?
```{r}
ggplot(X) + aes(x = age, y = vocab, color=nativeBorn) + geom_smooth() 
```
Create two different plots and identify the best-looking plot you can to examine the `vocab` variable by `educGroup`. Does there appear to be an association?
```{r}
ggplot(X) + aes(x = educGroup, y = vocab) + geom_boxplot()
```
Using the best-looking plot from the previous question, create the best looking overloading with variable `gender`. Does there appear to be an interaction of `gender` and `educGroup`?
```{r}
#TO-DO
```
Using facets, examine the relationship between `vocab` and `ageGroup`. You can drop year level `(Other)`. Are we getting dumber?
```{r}
#TO-DO
```
#Logistic Regression
Let's consider the Pima Indians Diabetes dataset from 1988:
```{r}
?MASS::Pima.tr
pima = na.omit(MASS::Pima.tr)
skimr::skim(pima)
y = ifelse(MASS::Pima.tr$type == "Yes", 1, 0)
X = cbind(1, MASS::Pima.tr[, 1 : 7])
```
Note the missing data. We will learn about how to handle missing data towards the end of the course. For now, replace, the missing data in the design matrix X with the average of the feature x_dot,j. You can check that this worked with the table commands at the end of the chunk:
```{r}
table(X$bp, useNA = "always")
table(X$skin, useNA = "always")
table(X$bmi, useNA = "always")
```
Now let's fit a log-odds linear model of y=1 (type is "diabetic") on just the `glu` variable. Use `optim` to fit the model.
```{r}
x = pima$glu
log_logistic_prob = function(w){
-sum(-y*log(1+exp(-w[1]-w[2]*x))-(1-y)*log(1+exp(w[1]+w[2]*x)))
}
optim(c(0,0), log_logistic_prob)
# y = ifelse(MASS::Pima.tr2$type == "Yes", 1, 0)
# x = pima$glu
# log_logistic_prod = function(w){
#   -sum(-y*log(1+exp(-w[1] - w[2] * x)) - (1-y) * log(1+exp(w[1] + w[2] * x)))
# }
# # sum(-y*log(1+exp(-w_0 - w_1 * x)) - (1-y) * log(1+exp(w_0 + w_1 * x)))
# optim(c(0, 0), log_logistic_prod)
```
Masters students: write a `fit_logistic_regression` function which takes in X, y and returns b which uses the optimization routine.
```{r}
fit_logistic_regression = function(X, y){
  b = #TO-DO
  b
}
```
Run a logistic regression of y=1 (type is "diabetic") on just the `glu` variable using R's built-in function and report b_0, b_1.
```{r}
b = coef(glm(y ~ x, family = "binomial"))
```
Comment on how close the results from R's built-in function was and your optimization call.

Quite close. 

Interpret the value of b_1 from R's built-in function.
One unit increase in x resutls in a 0.04 increase in log-odds of having diabetes
Interpret the value of b_0 from R's built-in function.

#to-do

Plot the probability of y=1 from the minimum value of `glu` to the maximum value of `glu`.
```{r}
min(x)
max(x)
res = 0.1
x_stars = seq(from = min(x), to = max(x), by = res)
log_odds_hat = cbind(1, x_stars) %*% b
p_hat = 1 / (1+exp(-log_odds_hat))
pacman::p_load(ggplot2)
ggplot(data.frame(glucose = x_stars, pred_prob_diab = p_hat)) + 
  aes(x = glucose, y = pred_prob_diab) +
  geom_line()
```
Run a logistic regression of y=1 (type is "diabetic") on all variables using R's built-in function and report the b vector.
```{r}
coef(glm(y ~ X[, "glu"], family = "binomial"))
```
Predict the probability of diabetes for someone with a blood sugar of 150.
```{r}
#TO-DO
```
For 100 people with blood sugar of 150, what is the probability more than 75 of them have diabetes? (You may need to review 241 to do this problem).
```{r}
#TO-DO
```
Plot the in-sample log-odds predictions (y-axis) versus the real response values (x-axis).
```{r}
p_hat = glm(y ~ X[, "glu"], family = "binomial")$fitted.values
log_odds_hat = log(p_hat / (1 - p_hat))
ggplot(data.frame(log_odds_hat = log_odds_hat, has_diabetes = pima$type)) + 
  aes(x = has_diabetes, y = log_odds_hat) + 
  geom_boxplot()
```
Plot the in-sample probability predictions (y-axis) versus the real response values (x-axis).
```{r}
ggplot(data.frame(p_hat = p_hat, has_diabetes = pima$type)) + 
  aes(x = has_diabetes, y = p_hat) + 
  geom_boxplot()
```
Comment on how well you think the logistic regression performed in-sample.
Not very well. 
Calculate the in-sample Brier score.
```{r}
#TO-DO
```
Calculate the in-sample log-scoring rule.
```{r}
#TO-DO
```
Run a probit regression of y=1 (type is "diabetic") on all variables using R's built-in function and report the b vector.
```{r}
#TO-DO
```
Does the weight estimates here in the probit fit have different signs than the weight estimates in the logistic fit? What does that mean?
#TO-DO
Plot the in-sample probability predictions (y-axis) versus the real response values (x-axis).
```{r}
#TO-DO
```
Calculate the in-sample Brier score.
```{r}
#TO-DO
```
Calculate the in-sample log-scoring rule.
```{r}
#TO-DO
```
Which model did better in-sample?
#TO-DO
Compare both model oos using the Brier score and a test set with 1/3 of the data.
```{r}
#TO-DO
```
Which model did better oos?
#TO-DO

